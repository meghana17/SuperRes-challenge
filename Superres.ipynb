{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Superres.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZZQtCAUQ2E-"
      },
      "source": [
        "import random \r\n",
        "import glob \r\n",
        "import subprocess \r\n",
        "import os \r\n",
        "from PIL import Image\r\n",
        "import numpy as np \r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "from tensorflow.keras.callbacks import Callback\r\n",
        "import wandb \r\n",
        "from wandb.keras import WandbCallback\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NfXaCFLQ92X"
      },
      "source": [
        "run = wandb.init(project='superres') \r\n",
        "config = run.config\r\n",
        "\r\n",
        "config.num_epochs    = 50 \r\n",
        "config.batch_size    = 32 #16, 64, 128\r\n",
        "config.input_height  = 32\r\n",
        "config.input_width   = 32\r\n",
        "config.output_height = 256\r\n",
        "config.output_width  = 256\r\n",
        "\r\n",
        "val_dir = '/content/drive/MyDrive/superres/data/test'\r\n",
        "train_dir = '/content/drive/MyDrive/superres/data/train'\r\n",
        "\r\n",
        "# automatically get the data if it doesn't exist\r\n",
        "if not os.path.exists(\"data\"):\r\n",
        "    print(\"Downloading flower dataset...\")\r\n",
        "    subprocess.check_output(\r\n",
        "        \"mkdir data && curl https://storage.googleapis.com/wandb/flower-enhance.tar.gz | tar xzf - -C data\", shell=True)\r\n",
        "\r\n",
        "config.steps_per_epoch = len(\r\n",
        "    glob.glob(train_dir + \"/*-in.jpg\")) // config.batch_size\r\n",
        "config.val_steps_per_epoch = len(\r\n",
        "    glob.glob(val_dir + \"/*-in.jpg\")) // config.batch_size\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbkap1x3Q9-1"
      },
      "source": [
        "def image_generator(batch_size, img_dir):\r\n",
        "    \"\"\"A generator that returns small images and large images.  DO NOT ALTER the validation set\"\"\"\r\n",
        "    input_filenames = glob.glob(img_dir + \"/*-in.jpg\")\r\n",
        "    counter = 0\r\n",
        "    random.shuffle(input_filenames)\r\n",
        "    while True:\r\n",
        "        small_images = np.zeros(\r\n",
        "            (batch_size, config.input_width, config.input_height, 3))\r\n",
        "        large_images = np.zeros(\r\n",
        "            (batch_size, config.output_width, config.output_height, 3))\r\n",
        "        if counter+batch_size >= len(input_filenames):\r\n",
        "            counter = 0\r\n",
        "        for i in range(batch_size): \r\n",
        "            img = input_filenames[counter + i]\r\n",
        "            small_images[i] = np.array(Image.open(img)) / 255.0\r\n",
        "            large_images[i] = np.array(\r\n",
        "                Image.open(img.replace(\"-in.jpg\", \"-out.jpg\"))) / 255.0\r\n",
        "        yield (small_images, large_images)\r\n",
        "        counter += batch_size\r\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCFV2AePRRZ1"
      },
      "source": [
        "def perceptual_distance(y_true, y_pred):\r\n",
        "    \"\"\"Calculate perceptual distance, DO NOT ALTER\"\"\"\r\n",
        "    y_true *= 255\r\n",
        "    y_pred *= 255\r\n",
        "    rmean = (y_true[:, :, :, 0] + y_pred[:, :, :, 0]) / 2\r\n",
        "    r = y_true[:, :, :, 0] - y_pred[:, :, :, 0]\r\n",
        "    g = y_true[:, :, :, 1] - y_pred[:, :, :, 1]\r\n",
        "    b = y_true[:, :, :, 2] - y_pred[:, :, :, 2]\r\n",
        "\r\n",
        "    return K.mean(K.sqrt((((512+rmean)*r*r)/256) + 4*g*g + (((767-rmean)*b*b)/256)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpRuHR7mN2mq"
      },
      "source": [
        "class ImageLogger(Callback):\r\n",
        "    def on_epoch_end(self, epoch, logs):\r\n",
        "        preds = self.model.predict(in_sample_images)\r\n",
        "        in_resized = []\r\n",
        "        for arr in in_sample_images:\r\n",
        "            # Simple upsampling\r\n",
        "            in_resized.append(arr.repeat(8, axis=0).repeat(8, axis=1))\r\n",
        "        wandb.log({\r\n",
        "            \"examples\": [wandb.Image(np.concatenate([in_resized[i] * 255, o * 255, out_sample_images[i] * 255], axis=1)) for i, o in enumerate(preds)]\r\n",
        "        }, commit=False)\r\n",
        "        if logs['val_perceptual_distance'] < 45:\r\n",
        "          self.model.stop_training = True\r\n",
        "        if epoch==30:\r\n",
        "          K.set_value(self.model.optimizer.lr, 5e-5)\r\n",
        "        if epoch==300:\r\n",
        "          K.set_value(self.model.optimizer.lr, 2e-5)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX_I7ymoRUkk"
      },
      "source": [
        "train_generator = image_generator(config.batch_size, train_dir)\r\n",
        "val_generator ``= image_generator(config.batch_size, val_dir)\r\n",
        "in_sample_images, out_sample_images = next(val_generator)\r\n",
        "\r\n",
        "feature_size   = 256\r\n",
        "num_layers     = 32\r\n",
        "scaling_factor = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkWjcAdSSZ28"
      },
      "source": [
        "def resBlock(x, channels = 256):\r\n",
        "  tmp = layers.Conv2D(channels, (3,3), padding=='same', activation='relu')(x)\r\n",
        "  tmp = layers.Conv2D(channels, (3,3), padding='same')(tmp)\r\n",
        "  tmp = layers.Conv2D(lambda sf: sf+scaling_factor)(tmp)\r\n",
        "\r\n",
        "  return layers.Add()([x,tmp])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvtxNWlJS9wg"
      },
      "source": [
        "def Generator(lr_shape=(32,32,3)):\r\n",
        "  gen_input = layers.Input(shape = lr_shape, dtype='float32')\r\n",
        "  conv_1 = layers.Conv2D(feature_size, (3,3), padding='same', activation='relu')(gen_input)\r\n",
        "  x = conv_1\r\n",
        "\r\n",
        "  for i in range(num_layers):\r\n",
        "    x = resBlock(x)\r\n",
        "  \r\n",
        "  x = layers.Conv2D(feature_size, (3,3), padding='same', activation='relu')(x)\r\n",
        "  tmp = layers.Add()([x, conv_1])\r\n",
        "  tmp = layers.Conv2D(feature_size, (3,3), padding='same', activation='relu')(tmp)\r\n",
        "  tmp = layers.Conv2D(3*(8**2), (3,3), padding='same')(tmp)\r\n",
        "  output = layers.Lambda(lambda x: tf.depth_to_space(x, 8), name='gen_output'(tmp))\r\n",
        "\r\n",
        "  gen_model = Model(inputs=gen_input, outputs=output)\r\n",
        "\r\n",
        "  return gen_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSac4HWObtLx"
      },
      "source": [
        "def discriminator_block(model, filters, kernel_size, strides):\r\n",
        "  model = layers.Conv2D(filters=filters, kernel_size = kernel_size, strides = strides, padding = 'same')(model)\r\n",
        "  model = layers.BatchNormalization(momentum = 0.5)(model)\r\n",
        "  model = layers.LeakyReLU(alpha = 0.2)(model)\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGPpPwekcPLX"
      },
      "source": [
        "def Discriminator(hr_shape=(256,256,3)):\r\n",
        "  dis_input = layers.Input(shape = hr_shape, dtype='float32')\r\n",
        "  model = layers.Conv2D(filters=64, kernel_size = 3, strides = 1, padding = 'same')(dis_input)\r\n",
        "  model = layers.LeakyReLU(alpha = 0.2)(model)\r\n",
        "\r\n",
        "  model = discriminator_block(model, 64, 3, 2)\r\n",
        "  model = discriminator_block(model, 128, 3, 1)\r\n",
        "  model = discriminator_block(model, 128, 3, 2)\r\n",
        "  model = discriminator_block(model, 256, 3, 1)\r\n",
        "  model = discriminator_block(model, 256, 3, 2)\r\n",
        "  model = discriminator_block(model, 512, 3, 1)\r\n",
        "  model = discriminator_block(model, 512, 3, 2)\r\n",
        "  model = discriminator_block(model, 512, 3, 2)\r\n",
        "\r\n",
        "  model = layers.Flatten()(model)\r\n",
        "  model = layers.Dense(512)(model)\r\n",
        "  model = layers.LeakyReLU(alpha = 0.2)(model)\r\n",
        "  model = layers.Dense(1)(model)\r\n",
        "\r\n",
        "  discriminator_model = Model(inputs = dis_input, outputs = model)\r\n",
        "\r\n",
        "  return discriminator_model\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h99jrzGS4K_8"
      },
      "source": [
        "vgg = applications.vgg19.VGG19(include_top=False, weights='imagenet', input_shape=(256,256,3))\r\n",
        "vgg.trainable = False\r\n",
        "\r\n",
        "for l in vgg.layers:\r\n",
        "  l.trainable = False\r\n",
        "\r\n",
        "#vgg_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block5_conv4').output)\r\n",
        "#vgg_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block4_conv1').output)\r\n",
        "vgg_model = Model(inputs=vgg.input, outputs=vgg.get_layer('block2_conv2').output)\r\n",
        "vgg_model.trainable = False\r\n",
        "vgg_model.compile(loss='mse', optimizer=Adam(lr=1e-04, epsilon=1e-08))\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UVzfomv5goP"
      },
      "source": [
        "def vgg_loss(y_true,y_pred):\r\n",
        "  y_true += 255\r\n",
        "  y_pred += 255\r\n",
        "  #y_pred = K.clip(y_pred, 0.0, 255.0)\r\n",
        "  #return K.mean(K.square(vgg_model(y_true) - vgg_model(y_pred)))\r\n",
        "  return K.mean(K.abs(vgg_model(y_true) - vgg_model(y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONPPRuJq6Hr4"
      },
      "source": [
        "def BCEwithLogitLoss(y_true, y_pred):\r\n",
        "  pred_g_fake = y_pred[:config.batch_size]\r\n",
        "  pred_d_real = y_pred[config.batch_size:]\r\n",
        "  # not sure if this works ???? Stack overflow lol\r\n",
        "  #t1 = pred_g_fake - K.sigmoid(pred_g_fake - K.mean(pred_d_real))\r\n",
        "  #t2 = pred_d_real - K.sigmoid(pred_d_real - K.mean(pred_g_fake))\r\n",
        "  t1 = pred_g_fake - K.mean(pred_d_real)\r\n",
        "  t2 = pred_d_real - K.mean(pred_g_fake)\r\n",
        "  bce = (K.binary_crossentropy(y_true[:config.batch_size], t1, from_logits=True) + K.binary_crossentropy(y_true[config.batch_size:]))\r\n",
        "\r\n",
        "  return K.mean(bce)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G6GqrxDRZ71"
      },
      "source": [
        "def get_gan_network(discriminator, lr_shape, generator, hr_shape):\r\n",
        "  discriminator.trainable = False\r\n",
        "  lr_images   = layers.Input(shape=lr_shape, name='lr_input')\r\n",
        "  fake_images = generator(lr_images)\r\n",
        "  real_images = layers.Input(shape=hr_shape, name='hr_input')\r\n",
        "  comined_img = layers.Lambda(lambda x: K.concatenate([x[0], x[1]], axis=0), name='fr_combine')([fake_images, real_images])\r\n",
        "  #print(combined_img)\r\n",
        "  gan_output = discriminator(combined_img)\r\n",
        "  #print(gan_output)\r\n",
        "  gan = Model(inputs = [lr_images, hr_images], outputs = [fake_images, gan_output])\r\n",
        "\r\n",
        "  gan.compile(loss=[vgg_loss, BCEwithLogitLoss], loss_weights=[2e-3, 1e-3], optimizer=Adam(lr=2.5e-05, epsilon=1e-08))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  return gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjWVsna9TRiQ"
      },
      "source": [
        "adam = Adam(lr = .5e-05, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nmeVZFsU9vG"
      },
      "source": [
        "real_data_Y = K.ones((config.batch_size,))\r\n",
        "fake_data_Y = K.zeros((config.batch_size,))\r\n",
        "\r\n",
        "dis_label = K.concatenate([fake_data_Y, real_data_Y], axis=0)\r\n",
        "gan_label = K.concatenate([real_data_Y, fake_data_Y], axis=0)\r\n",
        "\r\n",
        "Model_save_path = '/content/drive/MyDrive/superres/model_save_esrgan_optv4/'\r\n",
        "if not os.path.exists(Model_save_path):\r\n",
        "  os.makedirs(Model_save_path)\r\n",
        "\r\n",
        "'''\r\n",
        "generator = Generator(lr_shape=(32,32,3))\r\n",
        "discriminator = Discriminator(hr_shape=(256,256,3))\r\n",
        "generator.compile(loss=perceptual_distance, optimizer=adam)\r\n",
        "discriminator.compile(loss=BCEwithLogitLoss, optimizer=Adam(lr=1e-04, epsilon=1e-08))\r\n",
        "gan = get_gan_network(discriminator, (32,32,3), generator, (256,256,3))\r\n",
        "\r\n",
        "print(generator.summary())\r\n",
        "print(gan.summary())\r\n",
        "'''\r\n",
        "\r\n",
        "generator = Generator(lr_shape=(32,32,3))\r\n",
        "discriminator = Discriminator(hr_shape=(256,256,3))\r\n",
        "generator.compile(loss=perceptual_distance, optimizer=Adam(lr=1e-04, epsilon=1e-08))\r\n",
        "discriminator.compile(loss=BCEwithLogitLoss, optimizer=Adam(lr=1e-04, epsilon=1e-08))\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC-1Wwtxasq_"
      },
      "source": [
        "if not os.path.exists(Model_save_path+'gen_model.h5'):\r\n",
        "  for epoch in range(1,501):\r\n",
        "    print('EPOCH', epoch)\r\n",
        "    for _ in range(config.steps_per_epoch):\r\n",
        "      train_lr_images, train_hr_images = next(train_generator)\r\n",
        "      d_loss = 0\r\n",
        "\r\n",
        "      if epoch%2 == 0:\r\n",
        "        gen_image_sr = generator.predict(train_lr_images)\r\n",
        "        dis_input = K.concatenate([gen_images_sr, train_hr_images, axis=0])\r\n",
        "        d_loss = discriminator.train_on_batch(dis_input, dis_label)\r\n",
        "\r\n",
        "    if epoch%2 == 0:\r\n",
        "      in_sample_images, out_sample_images = next(val_generator)\r\n",
        "      pred = generator.predict(in_sample_images)\r\n",
        "      val_perceptual = perceptual_distance(np.array(out_sample_images), pred)\r\n",
        "      print('val_perceptual = ', val_perceptual)\r\n",
        "      generator.save(Model_save_path+'gen_model.h5', overwrite=True)\r\n",
        "      discriminator.save(Model_save_path+'dis_model.h5', overwrite=True)\r\n",
        "  \r\n",
        "\r\n",
        "  \r\n",
        "else:\r\n",
        "  discrininator = load_model(Model_save_path+'dis_model.h5', custom_objects={'BCEwithLogitLoss':BCEwithLogitLoss})\r\n",
        "  discriminator.compile(loss=BCEwithLogitLoss, optimizer=Adam(lr=2.5e-05, epsilon=1e-08))\r\n",
        "\r\n",
        "  generator = load_model(Model_save_path+'gen_model.h5', custom_objects={'perceptual_distance':perceptual_distance, 'tf':tf})\r\n",
        "  generator.compile(loss=perceptual_distance, optimizer=adam)\r\n",
        "\r\n",
        "  for epoch in range(1,501):\r\n",
        "    print('EPOCH', epoch)\r\n",
        "    for _ in range(config.steps_per_epoch):\r\n",
        "      train_lr_images, train_hr_images = next(train_generator)\r\n",
        "      d_loss = 0\r\n",
        "\r\n",
        "      if epoch%2 == 0:\r\n",
        "        gen_image_sr = generator.predict(train_lr_images)\r\n",
        "        dis_input = K.concatenate([gen_images_sr, train_hr_images, axis=0])\r\n",
        "        d_loss = discriminator.train_on_batch(dis_input, dis_label)\r\n",
        "\r\n",
        "    if epoch%2 == 0:\r\n",
        "      in_sample_images, out_sample_images = next(val_generator)\r\n",
        "      pred = generator.predict(in_sample_images)\r\n",
        "      val_perceptual = perceptual_distance(np.array(out_sample_images), pred)\r\n",
        "      print('val_perceptual = ', val_perceptual)\r\n",
        "      generator.save(Model_save_path+'gen_model.h5', overwrite=True)\r\n",
        "      discriminator.save(Model_save_path+'dis_model.h5', overwrite=True)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JARo4VO_edwZ"
      },
      "source": [
        "val_sequence = []\r\n",
        "if not os.path.exists(Model_save_path+'gen_model_gan.h5'):\r\n",
        "  dis_saved = load_model(Model_save_path+'dis_model.h5', custom_objects={'BCEwithLogitLoss':BCEwithLogitLoss})\r\n",
        "  dis_saved.compile(loss=BCEwithLogitLoss, optimizer=Adam(lr=2.5e-05, epsilon=1e-08))\r\n",
        "  gen_saved = load_model(Model_save_path+'gen_model.h5', custom_objects={'vgg_loss': vgg_loss, 'perceptual_distance':perceptual_distance, 'tf':tf})\r\n",
        "  gan = get_gan_network(dis_saved, (32,32,3), gen_Saved, (256,256,3))\r\n",
        "\r\n",
        "  for epoch in range(1,501):\r\n",
        "    print('EPOCH', epoch)\r\n",
        "    for _ in range(config.steps_per_epoch):\r\n",
        "      train_lr_images, train_hr_images = next(train_generator)\r\n",
        "      d_loss = 0\r\n",
        "\r\n",
        "      gen_image_sr = gen_saved.predict(train_lr_images)\r\n",
        "      dis_saved.trainable = True\r\n",
        "      dis_input = K.concatenate([gen_images_sr, train_hr_images, axis=0])\r\n",
        "      d_loss = dis_saved.train_on_batch(dis_input, dis_label)\r\n",
        "\r\n",
        "      dis_saved.trainable = False\r\n",
        "      loss_gan = gan.train_on_batch([train_lr_images, train_hr_images], [train_hr_images, gan_label])\r\n",
        "\r\n",
        "    in_sample_images, out_sample_images = next(val_generator)\r\n",
        "    pred = gen_saved.predict(in_sample_images)\r\n",
        "    tmp_val = perceptual_distance(np.array(out_sample_images), pred)\r\n",
        "    print('val_perceptual = ', val_perceptual)\r\n",
        "    val_sequence.append(tmp_val)\r\n",
        "    gen_saved.save(Model_save_path+'gen_model_gan.h5', overwrite=True)\r\n",
        "    dis_saved.save(Model_save_path+'dis_model_gan.h5', overwrite=True)\r\n",
        "\r\n",
        "\r\n",
        "else:\r\n",
        "  dis_saved = load_model(Model_save_path+'dis_model_gan.h5', custom_objects={'BCEwithLogitLoss':BCEwithLogitLoss})\r\n",
        "  dis_saved.compile(loss=BCEwithLogitLoss, optimizer=Adam(lr=2.5e-05, epsilon=1e-08))\r\n",
        "\r\n",
        "  gen_saved= load_model(Model_save_path+'gen_model_gan.h5', custom_objects={'vgg_loss': vgg_loss, 'perceptual_distance':perceptual_distance, 'tf':tf})\r\n",
        "  gan = get_gan_network(dis_Saved, (32,32,3), gen_saved, (256,256,3))\r\n",
        "\r\n",
        "  for epoch in range(1,251):\r\n",
        "    print('EPOCH', epoch)\r\n",
        "    for _ in range(config.steps_per_epoch):\r\n",
        "      train_lr_images, train_hr_images = next(train_generator)\r\n",
        "      d_loss = 0\r\n",
        "\r\n",
        "      gen_image_sr = gen_saved.predict(train_lr_images)\r\n",
        "      dis_saved.trainable = True\r\n",
        "      dis_input = K.concatenate([gen_images_sr, train_hr_images, axis=0])\r\n",
        "      d_loss = dis_saved.train_on_batch(dis_input, dis_label)\r\n",
        "\r\n",
        "      dis_saved.trainable = False\r\n",
        "      loss_gan = gan.train_on_batch([train_lr_images, train_hr_images], [train_hr_images, gan_label])\r\n",
        "\r\n",
        "    in_sample_images, out_sample_images = next(val_generator)\r\n",
        "    pred = gen_saved.predict(in_sample_images)\r\n",
        "    tmp_val = perceptual_distance(np.array(out_sample_images), pred)\r\n",
        "    print('val_perceptual = ', val_perceptual)\r\n",
        "    val_sequence.append(tmp_val)\r\n",
        "    gen_saved.save(Model_save_path+'gen_model_gan.h5', overwrite=True)\r\n",
        "    dis_saved.save(Model_save_path+'dis_model_gan.h5', overwrite=True)\r\n",
        "\r\n",
        "                         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0kp5ZCcjw0v"
      },
      "source": [
        "model = load_model(Model_save_path+\"gen_model_gan.h5\", custom_objects={'vgg_loss': vgg_loss, 'perceptual_distance':perceptual_distance, 'tf':tf} )\r\n",
        "model.trainable = False\r\n",
        "model.compile(loss='mae', optimizer=Adam(lr=2.5e-05, epsilon=1e-08), metrics=[perceptual_distance])\r\n",
        "print(model.summary())\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUPUExeLkfPV"
      },
      "source": [
        "model.fit_generator(image_generator(config.batch_size, train_dir),\r\n",
        "                    steps_per_epoch = config.steps_per_epoch,\r\n",
        "                    epochs = congif.num_epochs,\r\n",
        "                    callbacks = [ImageLogger(), WandbCallback()],\r\n",
        "                    validation_steps = config.val_steps_per_epoch,\r\n",
        "                    vaidation_data = val_generator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}